{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self,num_heads,key_dim,feature_dim,ff_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.multiheadatt = layers.MultiHeadAttention(num_heads=num_heads,key_dim=key_dim,\\\n",
    "        \tdropout=dropout)\n",
    "        self.feed_forward_layer = keras.Sequential([\n",
    "        \tlayers.Dense(ff_dim,activation='relu'),\\\n",
    "        \tlayers.Dense(feature_dim)])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\t\n",
    "\n",
    "    def call(self,input,training,attention_mask):\n",
    "    \tattention_output = self.multiheadatt(input,input,input,attention_mask=attention_mask)\n",
    "    \tattention_output = self.dropout1(attention_output,training=training)\n",
    "    \tout1 = self.layernorm1(input+attention_output)\n",
    "\n",
    "    \tffn_output = self.feed_forward_layer(out1)\n",
    "    \tffn_output = self.dropout2(ffn_output,training=training)\n",
    "    \tout2 = self.layernorm2(out1+ffn_output)\n",
    "    \treturn out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder_layer = EncoderLayer(num_heads=8,key_dim=512,feature_dim=512,ff_dim=2048,dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 43, 512), dtype=float32, numpy=\n",
       "array([[[-0.30807516, -0.7205544 ,  2.4360769 , ..., -0.5028709 ,\n",
       "         -0.36315203,  1.3812314 ],\n",
       "        [-0.9100085 , -0.753673  ,  0.17345531, ...,  0.3546194 ,\n",
       "         -0.78518265,  1.040365  ],\n",
       "        [ 1.2655596 ,  1.1582346 ,  0.2333653 , ..., -0.5825921 ,\n",
       "          0.1024247 ,  0.08858624],\n",
       "        ...,\n",
       "        [ 0.8774717 ,  0.97786987, -0.4663051 , ..., -0.01119186,\n",
       "          0.03093335,  0.3781778 ],\n",
       "        [ 1.5765157 ,  1.002756  ,  2.4507341 , ..., -0.33592287,\n",
       "          0.73702234,  0.83981174],\n",
       "        [ 1.4055518 , -0.5618804 ,  1.8225296 , ..., -0.19343297,\n",
       "          0.94391245,  0.84352016]],\n",
       "\n",
       "       [[ 0.91029596, -0.3154266 ,  1.4921741 , ...,  1.1360452 ,\n",
       "         -1.2503752 ,  1.2760714 ],\n",
       "        [ 0.9605291 ,  0.5872383 ,  2.6066182 , ...,  0.2145131 ,\n",
       "          0.44794998, -0.12872437],\n",
       "        [ 0.59091246, -0.49139056,  0.36990115, ..., -0.5641501 ,\n",
       "          1.6124548 , -0.7470893 ],\n",
       "        ...,\n",
       "        [-0.799545  , -0.64010525,  1.0217725 , ..., -0.21168019,\n",
       "         -1.6511579 , -1.5656039 ],\n",
       "        [ 1.6679817 , -0.5439717 ,  0.23715875, ...,  1.8082721 ,\n",
       "         -1.4979295 , -0.25208414],\n",
       "        [ 0.09428401, -1.8640815 ,  0.17765495, ...,  0.877831  ,\n",
       "         -0.12021649, -0.8267377 ]],\n",
       "\n",
       "       [[-0.2506359 , -1.4845783 ,  1.4206382 , ..., -0.53641766,\n",
       "          1.3671147 , -1.2920759 ],\n",
       "        [ 0.8606361 ,  0.44720396,  0.46709925, ..., -0.819324  ,\n",
       "         -0.2645092 , -0.22811344],\n",
       "        [ 0.14992635, -1.2470895 , -0.95537573, ...,  0.6174726 ,\n",
       "          0.2566687 , -1.2412138 ],\n",
       "        ...,\n",
       "        [ 0.6195551 ,  0.08231404,  0.82266736, ..., -0.4965463 ,\n",
       "          0.87286437,  0.19402531],\n",
       "        [-1.306922  , -1.7927779 , -0.51730895, ...,  0.7094905 ,\n",
       "          0.6728851 ,  0.19590637],\n",
       "        [-0.2228225 ,  0.4812814 ,  0.4886856 , ..., -0.4861524 ,\n",
       "         -0.9169591 ,  1.4657978 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.3307505 , -0.44762284,  0.05195779, ..., -0.42267668,\n",
       "         -1.4242144 , -0.08388545],\n",
       "        [-0.6499577 , -1.960238  ,  0.33222696, ...,  0.95003015,\n",
       "         -1.7378527 , -0.97067434],\n",
       "        [ 0.7011798 ,  0.33103415,  1.1457695 , ...,  0.974635  ,\n",
       "          0.7540687 , -0.70109725],\n",
       "        ...,\n",
       "        [ 2.721543  ,  1.0280802 ,  1.1476092 , ...,  1.6555948 ,\n",
       "         -0.8472719 ,  0.6460511 ],\n",
       "        [ 0.33847803, -0.01149723,  0.5945235 , ..., -2.0537086 ,\n",
       "         -0.9332344 , -0.13133183],\n",
       "        [ 0.82522124, -0.1380688 ,  0.6689311 , ...,  0.7787123 ,\n",
       "         -0.85630345,  0.04752899]],\n",
       "\n",
       "       [[-0.13839367,  0.8820043 ,  2.2342668 , ...,  0.45438588,\n",
       "         -1.1008413 ,  1.0220279 ],\n",
       "        [ 0.48822263,  0.37002814,  1.0963217 , ...,  0.73469293,\n",
       "         -1.3241056 ,  1.2891554 ],\n",
       "        [ 0.2894394 , -0.83650494, -0.2780469 , ..., -1.3420652 ,\n",
       "          0.90477186,  0.41867626],\n",
       "        ...,\n",
       "        [ 0.62014085,  0.1154941 ,  1.5089117 , ..., -0.8086951 ,\n",
       "         -1.2020335 ,  0.58916926],\n",
       "        [ 1.4171327 ,  0.63096255,  1.1705022 , ...,  0.83653164,\n",
       "          0.5587219 ,  1.6958663 ],\n",
       "        [ 1.5798247 , -1.4433897 , -0.33064157, ..., -1.3401212 ,\n",
       "         -0.9066747 , -1.0466508 ]],\n",
       "\n",
       "       [[ 1.3843364 , -1.1184573 ,  0.4137413 , ...,  0.00833031,\n",
       "         -0.8860064 , -1.2153459 ],\n",
       "        [ 1.212724  , -1.0185984 ,  0.44988522, ...,  0.39958254,\n",
       "         -1.3289638 , -0.5569377 ],\n",
       "        [ 1.3421817 , -2.0234168 ,  2.0763323 , ...,  0.97458816,\n",
       "          0.27948448, -0.4127462 ],\n",
       "        ...,\n",
       "        [ 1.0844564 , -2.0472827 ,  1.3683897 , ..., -0.0787243 ,\n",
       "         -1.3260182 , -0.06795937],\n",
       "        [ 0.9553983 , -2.3961914 ,  1.2958819 , ...,  1.6789198 ,\n",
       "         -0.6773538 ,  0.09232971],\n",
       "        [-0.1034768 ,  0.8896492 , -1.025937  , ...,  1.8987098 ,\n",
       "         -1.6000135 , -0.52418756]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self,num_heads,key_dim,feature_dim,ff_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.multiheadatt1 = layers.MultiHeadAttention(num_heads=num_heads,key_dim=key_dim,\\\n",
    "            dropout=dropout)\n",
    "        self.multiheadatt2 = layers.MultiHeadAttention(num_heads=num_heads,key_dim=key_dim,\\\n",
    "            dropout=dropout)\n",
    "\n",
    "        self.feed_forward_layer = keras.Sequential([\n",
    "            layers.Dense(ff_dim,activation='relu'),\\\n",
    "            layers.Dense(feature_dim)])\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        self.dropout3 = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self,input,encoder_outputs,training,look_ahead_mask,padding_mask):\n",
    "        attention_output1 = self.multiheadatt1(input,input,input,attention_mask=look_ahead_mask)\n",
    "        attention_output1 = self.dropout1(attention_output1,training=training)\n",
    "        out1 = self.layernorm1(input+attention_output1)\n",
    "\n",
    "        attention_output2 = self.multiheadatt2(value=encoder_outputs,key=encoder_outputs,\\\n",
    "                                               query=out1,attention_mask=padding_mask)\n",
    "        attention_output2 = self.dropout1(attention_output2,training=training)\n",
    "        out2 = self.layernorm2(out1+attention_output2)\n",
    "\n",
    "        ffn_output = self.feed_forward_layer(out2)\n",
    "        ffn_output = self.dropout2(ffn_output,training=training)\n",
    "        out2 = self.layernorm3(out1+ffn_output)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,num_encoder,num_heads,key_dim,feature_dim,ff_dim,dropout):\n",
    "        super().__init__()\n",
    "        patches = 100\n",
    "        self.num_encoder = num_encoder\n",
    "        self.pos_encoder = positional_encoding(patches,feature_dim)\n",
    "        self.encoder_layers = [EncoderLayer(num_heads,key_dim,feature_dim,ff_dim,dropout) \\\n",
    "                              for _ in range(num_encoder)]\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs,padding_mask,training=True):\n",
    "        inputs += self.pos_encoder\n",
    "        x = self.dropout(inputs,training=training)\n",
    "        for i in range(self.num_encoder):\n",
    "            x = self.encoder_layers[i](x,training=training,attention_mask=padding_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = Encoder(5,8,64,256,512,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder_layer_output = sample_encoder(\n",
    "    tf.random.uniform((1,100,256)),padding_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100, 256), dtype=float32, numpy=\n",
       "array([[[-1.1621993 , -1.2075881 ,  0.65932155, ...,  0.58714896,\n",
       "         -1.0473044 , -0.10137503],\n",
       "        [-0.90982187, -0.8698658 ,  0.4754767 , ...,  0.2795088 ,\n",
       "         -0.57237613, -1.2295765 ],\n",
       "        [-0.54445714, -1.5848739 ,  0.83412457, ..., -0.16353464,\n",
       "          0.19105615, -0.01728549],\n",
       "        ...,\n",
       "        [ 0.39641896, -1.9151562 ,  0.87335896, ..., -0.52655905,\n",
       "         -0.30221856,  1.3261619 ],\n",
       "        [-0.9286312 , -2.0479248 , -0.9718388 , ..., -0.35256937,\n",
       "         -0.6017862 ,  1.2943859 ],\n",
       "        [-0.49348664, -1.8590837 , -0.07375112, ..., -0.61502576,\n",
       "         -0.8946515 ,  0.68893313]]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_decoder,num_heads,key_dim,feature_dim,ff_dim,dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        patches = 100\n",
    "        self.num_decoder = num_decoder\n",
    "        self.pos_encoding = positional_encoding(patches, feature_dim)\n",
    "        self.dec_layers = [DecoderLayer(num_heads,key_dim,feature_dim,ff_dim,dropout)\n",
    "                           for _ in range(num_decoder)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "           look_ahead_mask, padding_mask):\n",
    "        x += self.pos_encoding\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_decoder):\n",
    "            x = self.dec_layers[i](x, enc_output, training,\n",
    "                                    look_ahead_mask, padding_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100, 256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_decoder=2, num_heads=4,\n",
    "                         key_dim=64,feature_dim=256,ff_dim=512,dropout=0.1)\n",
    "temp_input = tf.random.uniform((1,100,256))\n",
    "output = sample_decoder(temp_input,\n",
    "                          enc_output=sample_encoder_layer_output,\n",
    "                          training=False,\n",
    "                          look_ahead_mask=None,\n",
    "                          padding_mask=None)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100, 256), dtype=float32, numpy=\n",
       "array([[[ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00, ...,\n",
       "          1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "        [ 8.41470957e-01,  5.40302277e-01,  8.01961780e-01, ...,\n",
       "          1.00000000e+00,  1.07460786e-04,  1.00000000e+00],\n",
       "        [ 9.09297407e-01, -4.16146845e-01,  9.58144367e-01, ...,\n",
       "          1.00000000e+00,  2.14921558e-04,  1.00000000e+00],\n",
       "        ...,\n",
       "        [ 3.79607737e-01, -9.25147533e-01,  7.45109499e-01, ...,\n",
       "          9.99937236e-01,  1.04235075e-02,  9.99945700e-01],\n",
       "        [-5.73381901e-01, -8.19288254e-01, -8.97521228e-02, ...,\n",
       "          9.99935985e-01,  1.05309617e-02,  9.99944568e-01],\n",
       "        [-9.99206841e-01,  3.98208797e-02, -8.52340877e-01, ...,\n",
       "          9.99934673e-01,  1.06384167e-02,  9.99943435e-01]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder.pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
